{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Часть 3: Использование готовых MCP-серверов\n",
    "\n",
    "В этом примере мы используем готовый сервер `mcp-server-fetch` из официального репозитория Model Context Protocol.\n",
    "\n",
    "Вместо написания собственного кода для работы с веб-страницами, мы просто устанавливаем пакет и подключаем его."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "## Шаг 1: Импорты и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StdioTransport\n",
    "from langchain.agents import create_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\") or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "## Шаг 2: О сервере mcp-server-fetch\n",
    "\n",
    "Этот сервер из [официального репозитория MCP](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) предоставляет:\n",
    "\n",
    "- Загрузку веб-страниц через инструмент `fetch`\n",
    "- Конвертацию HTML в markdown для удобной обработки моделями\n",
    "- Параметры для контроля размера и чтения контента по частям\n",
    "\n",
    "Установка: `uvx mcp-server-fetch` (или через npm/pip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "## Шаг 3: Подключение готового сервера\n",
    "\n",
    "Создаем транспорт, который запускает сервер через `uvx`.\n",
    "Процесс подключения идентичен собственным серверам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    transport = StdioTransport(command=\"uvx\", args=[\"mcp-server-fetch\"])\n",
    "\n",
    "    async with Client(transport) as client:\n",
    "        print(\"Available tools:\")\n",
    "        tools_list = await client.list_tools()\n",
    "        for tool in tools_list:\n",
    "            print(f\"  - {tool.name}\")\n",
    "\n",
    "        tools = await load_mcp_tools(client.session)\n",
    "\n",
    "        llm = ChatOpenAI(model=BASE_MODEL, temperature=0)\n",
    "\n",
    "        agent = create_agent(llm, tools)\n",
    "\n",
    "        response = await agent.ainvoke(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    (\n",
    "                        \"user\",\n",
    "                        \"Fetch the content from https://docs.langchain.com/oss/python/langgraph/overview and summarize key points\",\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"\\nAgent response:\")\n",
    "        for message in response[\"messages\"]:\n",
    "            if (\n",
    "                message.type == \"ai\"\n",
    "                and hasattr(message, \"content\")\n",
    "                and message.content\n",
    "            ):\n",
    "                print(f\"\\n{message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "## Шаг 4: Запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {},
   "source": [
    "## Экосистема MCP и открытость\n",
    "\n",
    "MCP построен на принципах open source. Существуют репозитории с готовыми серверами:\n",
    "\n",
    "- [Docker Hub MCP](https://hub.docker.com/mcp/explore)\n",
    "- [Glama MCP Servers](https://glama.ai/mcp/servers)\n",
    "- Научные инструменты: [academia_mcp](https://github.com/IlyaGusev/academia_mcp)\n",
    "\n",
    "Вы можете:\n",
    "- Использовать готовые решения других разработчиков\n",
    "- Публиковать свои серверы для сообщества\n",
    "- Контрибутить в существующие проекты\n",
    "\n",
    "Открытость MCP означает возможность изучать/модифицировать любой сервер."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
