{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Часть 2: Создание MCP-сервера\n",
    "\n",
    "В этом примере мы создадим собственный MCP-сервер с использованием FastMCP и подключим его к агенту через STDIO транспорт.\n",
    "\n",
    "MCP решает проблемы обычного tool calling через стандартизацию и переиспользуемость."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32c89a-c442-4eaf-88ce-e0ff96848b8d",
   "metadata": {},
   "source": [
    "## Как это работает?\n",
    "\n",
    "МСп это по сути соглашение о том, в каком JSON-формате клиент (LLM) и сервер (ваш код) обмениваются просьбами выполнить функции, независимо от того, как они соединены (через HTTP или просто через консольный ввод-вывод)\n",
    "\n",
    "Задействуется три компонента:\n",
    "\n",
    "1. **MCP Server** - отдельная программа (локальный процесс через stdio или удалённый сервис через HTTP+SSE), которая предоставляет доступ к ресурсам, **инструментам** и промптам через стандартизированный MCP-интерфейс на базе JSON-RPC 2.0. Например, серверы для работы с файловой системой, историей коммитов, базами данных и т.д.\n",
    "2. **MCP Client** - модуль внутри хоста, который устанавливает соединение с конкретным MCP-сервером, обменивается JSON-RPC сообщениями и управляет жизненным циклом соединения.\n",
    "3. **Host** - приложение-контейнер (IDE-ассистент или ваш Python-процесс с мультиагентной системой), которое содержит LLM, управляет MCP-клиентами, координирует взаимодействие с пользователем и агрегирует контекст от разных серверов.\n",
    "\n",
    "Основная схема такая: Знакомство >> Запрос >> Ответ\n",
    "\n",
    "- На **первом** шаге мсп-клиент спрашивает у мсп-сервера что второй умеет. В ответ сервер показывает свое описание и список доступных инструментов. Выглядеть это может так:\n",
    "\n",
    "```json\n",
    "// Сообщение от MCP Server -> MCP Client\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"result\": {\n",
    "    \"tools\": [{\n",
    "        \"name\": \"calculator_arithmetic\",  // уникальное имя операции\n",
    "        \"title\": \"Calculator\",\n",
    "        \"description\": \"Perform mathematical calculations including basic arithmetic, trigonometric functions, and algebraic operations\",  // понятное для LLM описание\n",
    "        \"inputSchema\": {  // параметры метода (тула)\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"expression\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"Mathematical expression to evaluate (e.g., '2 + 3 * 4', 'sin(30)', 'sqrt(16)')\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"expression\"]\n",
    "        }\n",
    "      }\n",
    "  ]}\n",
    "}\n",
    "```\n",
    "\n",
    "    Затем хост передает такого вида сообщение вместе с воепросом в LLM. Затем сама модель решает что делать и знает, что у нее есть инструмент (в данном примере - калькулятор).\n",
    "\n",
    "- На **втором** шаге LLM анализирует вопрос и решает нужно ли для ответа вызвать тул. НАпример с `experession = \"2 + 2 * 2\"`. Хост через клиент отправляет запрос на сервер такого вида:\n",
    "\n",
    "```json\n",
    "// Сообщение от Host через MCP Client -> MCP Server\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 3,\n",
    "  \"method\": \"tools/call\",\n",
    "  \"params\": {\n",
    "    \"name\": \"calculator_arithmetic\",\n",
    "    \"arguments\": { \"expression\": \"2 + 2 * 2\" }\n",
    "  }\n",
    "}\n",
    "```\n",
    "    Это и есть вызов функции-тула\n",
    "\n",
    "- На **третьем** шаге сервер получает запрос, выполняет вычисления и возращает результат клиенту, а тот передает хосту, который затем направляет в LLM. Пример результата:\n",
    "\n",
    "```json\n",
    "// Сообщение от MCP Server -> MCP Client -> Host\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 3,\n",
    "  \"result\": {\n",
    "    \"content\": [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"8\"\n",
    "    }]\n",
    "  }\n",
    "}\n",
    "```\n",
    "    Теперь модель решает готова ли она дать окончательный ответ.\n",
    "\n",
    "> **Важно**: информация о подключенных мсп-серверах и мета-информация из них съедают контекст LLM. Нужно быть аккуратнее с описанием серверов и тулов. Также, не стоит расчитывать, что дав модели 100 мсп-серов вы будете получать хороший результат, скорее вы просто запутаете модель или заполните ей контекстое окно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af55ca-e99a-4116-ace1-6ccc9e7459a8",
   "metadata": {},
   "source": [
    "## Какие использовать библиотеки?\n",
    "\n",
    "В данном примере рассмотрим [FastMCP](https://gofastmcp.com/getting-started/welcome) (обертка над Starlette), тк эта библиотека активно развивается и поддерживает много фич. Но главное здесь - соблюдать протокол. Поэтому можно использовать и другие фреймворки, включая [оффициальную имплементацию MCP](https://github.com/modelcontextprotocol/python-sdk). **В том числе на других языках.** (помимо питон довольно популярен js/ts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "## Шаг 1: Импорты и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastmcp import Client\n",
    "from langchain.agents import create_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\") or \"qwen/qwen3-235b-a22b-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "## Шаг 2: Структура MCP-сервера\n",
    "\n",
    "Наш сервер находится в файле `server_stdio.py`. Он использует FastMCP и предоставляет два инструмента:\n",
    "\n",
    "- `convert_document` - конвертирует документы в markdown\n",
    "- `analyze_document` - возвращает статистику о документе\n",
    "\n",
    "Сервер запускается как отдельный процесс и общается через STDIO транспорт."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "## Шаг 3: Подключение к серверу\n",
    "\n",
    "Создаем клиент, который подключается к нашему серверу через STDIO.\n",
    "Сервер автоматически предоставляет список своих инструментов через `tools/list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    server_script = Path(\"server_stdio.py\")\n",
    "\n",
    "    async with Client(str(server_script)) as client:\n",
    "        print(\"Available tools:\")\n",
    "        tools_list = await client.list_tools()\n",
    "        for tool in tools_list:\n",
    "            print(f\"  - {tool.name}\")\n",
    "\n",
    "        tools = await load_mcp_tools(client.session)\n",
    "\n",
    "        llm = ChatOpenAI(model=BASE_MODEL, temperature=0)\n",
    "\n",
    "        agent = create_agent(llm, tools)\n",
    "\n",
    "        test_pdf = Path(\"../../data/test.pdf\")\n",
    "\n",
    "        response = await agent.ainvoke(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    (\n",
    "                        \"user\",\n",
    "                        f\"Analyze the document structure at {test_pdf} and create laconic summary of the text.\",\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\"Agent response:\")\n",
    "        for message in response[\"messages\"]:\n",
    "            if (\n",
    "                message.type == \"ai\"\n",
    "                and hasattr(message, \"content\")\n",
    "                and message.content\n",
    "            ):\n",
    "                print(f\"\\n{message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "## Шаг 4: Запуск примера\n",
    "\n",
    "Выполним асинхронную функцию main для взаимодействия с сервером."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cce6a46-cb57-42c5-8877-b25610768fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "BYtC",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "  - convert_document\n",
      "  - analyze_document\n",
      "Agent response:\n",
      "\n",
      "The manul (Otocolobus manul) is a small wild cat adapted to cold, arid steppe and semi-desert ecosystems across Central and Western Asia. It exhibits unique morphological and behavioral traits, including dense fur for insulation, cryptic coloration for camouflage, and reliance on rocky outcrops and burrows for shelter. The species is solitary, with large home ranges (up to 207 km² for males), low population densities (4–8/100 km² in Mongolia), and seasonal breeding tied to photoperiod. It primarily preys on pikas and small rodents, hunting at dawn and dusk using ambush, stalking, or flushing techniques.\n",
      "\n",
      "Manuls face high kitten mortality and threats from predation (especially by raptors and domestic dogs), habitat fragmentation, and human activities. Their survival depends on access to den sites, which are critical for reproduction and protection. Despite a broad distribution, populations are patchy and declining in parts of the range, particularly in the southwest. Conservation efforts must address habitat connectivity, prey availability, and human-related mortality to ensure long-term survival.\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9faa4-006f-4305-a60c-84994afb16b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
