# Часть 1: Основы Tool Calling для LLM-агентов

## Введение

В этой части мастер-класса мы разберем, как языковые модели могут вызывать внешние функции и взаимодействовать с окружением. Вы узнаете о механизме tool calling на фундаментальном уровне.

## Все основывается на паттерне структурированного вывода

Прежде чем разбирать работу с инструментами, важно понять ключевую концепцию - структурированный вывод. По умолчанию языковая модель возвращает обычный текст, который удобен для чтения человеком, но плох для программной обработки. Структурированный вывод - это когда модель возвращает данные в строго определенном формате, например JSON. Именно это позволяет автоматически парсить ответ модели, извлекать нужные поля и на их основе принимать решения - например, какую функцию вызвать и с какими параметрами. Без структурированного вывода механизм tool calling просто не смог бы работать.

Работу с инструментами можно разбить на три ключевых этапа:

1. **Определение инструмента** - описание возможностей
2. **Вызов инструмента** - выбор и вызов модель
3. **Возврат результата** - обработка и передача данных обратно модели

На каждом из этих этапов используется структурированный вывод.

## Этап 1: Определение инструмента

Работа с инструментами основывается на том, что мы в системный промпт модели или в промпт пользователя добавляем информацию об инструментах, которые модель может вызывать в данный момент.

**Где размещать описание инструментов:**

- **Системный промпт** - для статического набора инструментов
- **Промпт пользователя** - если нужно динамически изменять доступные инструменты

Инструмент представляет собой отдельную секцию в промпте. В этой секции может быть один или несколько инструментов. Чаще всего инструмент описывается в виде JSON с примерно таким набором предопределенных полей:

```json
{
  "name": "get_weather",
  "description": "Получает текущую погоду для указанного города",
  "input_schema": {
    "type": "object",
    "properties": {
      "city": {
        "type": "string",
        "description": "Название города"
      },
      "units": {
        "type": "string",
        "enum": ["celsius", "fahrenheit"],
        "description": "Единицы измерения температуры"
      }
    },
    "required": ["city"]
  }
}
```

**Почему это работает?**

LLM на этапе fine-tuning обучалась вызову инструментов по заданному протоколу. Модель "понимает", что когда она видит такое описание, она может использовать этот инструмент для решения задачи.

Таким образом, каждый инструмент описывается как JSON-схема с ключевыми параметрами:

- **name** - имя инструмента
- **description** - описание работы (зачем использовать, в каких ситуациях)
- **input_schema** - требуемые и опциональные параметры инструмента

### Пример: определение нескольких инструментов

```python
tools = [
    {
        "name": "get_weather",
        "description": "Получает текущую погоду для города",
        "input_schema": {
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "Название города"}
            },
            "required": ["city"]
        }
    },
    {
        "name": "calculate",
        "description": "Выполняет математические вычисления",
        "input_schema": {
            "type": "object",
            "properties": {
                "expression": {"type": "string", "description": "Математическое выражение"}
            },
            "required": ["expression"]
        }
    }
]
```

## Этап 2: Вызов инструмента

Вызов происходит схожим образом с определением - за счет структурированного вывода от LLM. Модель возвращает JSON, в котором указывает инструмент и параметры для его вызова.

**Важно понимать:** сам по себе JSON от модели не вызывает никакую функцию. Это просто структурированный текст. Нужен отдельный интерпретатор, который:

1. Распарсит JSON от модели
2. Найдет соответствующую Python-функцию
3. Подставит параметры
4. Выполнит функцию
5. Вернет результат

### Пример полного цикла

```python
# 1. Модель возвращает структурированный вывод
model_response = {
    "tool_use": {
        "id": "call_123",
        "name": "get_weather",
        "input": {"city": "Москва"}
    }
}

# 2. Интерпретатор находит и вызывает функцию
def get_weather(city: str) -> str:
    # Логика получения погоды (API, база данных и т.д.)
    return f"Температура в {city}: +5°C, облачно"

# 3. Выполнение
tool_name = model_response["tool_use"]["name"]
tool_input = model_response["tool_use"]["input"]

# Интерпретатор вызывает функцию
result = get_weather(**tool_input)
# Результат: "Температура в Москве: +5°C, облачно"

# 4. Результат отправляется обратно модели
tool_result = {
    "tool_result": {
        "call_id": "call_123",
        "content": result
    }
}
```

### Цикл взаимодействия

На этом этапе происходит общение модели со средой:

1. Модель получает запрос пользователя
2. Модель анализирует доступные инструменты
3. Модель возвращает вызов инструмента (structured output)
4. Интерпретатор выполняет функцию
5. Результат добавляется в историю диалога
6. Модель получает результат и может:
   - Вызвать еще один инструмент
   - Вернуть окончательный ответ пользователю
   - Продолжить пока не достигнет максимального количества итераций

**Хорошая новость:** Вам не нужно писать интерпретатор самостоятельно. Агентские фреймворки (LangChain, LangGraph, Pydantic AI) предоставляют готовый функционал работы с инструментами.

Результаты вызовов сохраняются в историю сообщений, формируя контекст для следующих шагов модели.

## Этап 3: Возврат результата

Результат выполнения инструмента - это обычное сообщение, которое передается обратно модели. Оно не обязательно должно быть структурированным.

Python-функция может вернуть:

- Простой текст: `"Температура: +5°C"`
- JSON: `{"temperature": 5, "condition": "cloudy"}`
- Сложные объекты (которые сериализуются в строку)

**Рекомендация:** Желательно возвращать структурированные данные (JSON), так модели проще их обрабатывать и извлекать нужную информацию.

### Пример с разными типами результатов

```python
# Простой текстовый результат
def get_time() -> str:
    return "Текущее время: 14:30"

# Структурированный результат
def get_weather(city: str) -> dict:
    return {
        "city": city,
        "temperature": 5,
        "condition": "облачно",
        "humidity": 75
    }

# Модель получает результат и может его проанализировать:
# "В Москве сейчас облачно, температура +5°C, влажность 75%"
```

## Ограничения текущего подхода

При работе с инструментами напрямую (без стандартизации) возникают проблемы:

### 1. Отсутствие переиспользуемости

Инструмент, написанный для одной модели или фреймворка, не работает с другими без переписывания.

### 2. Нет стандарта обнаружения

Каждый раз нужно вручную регистрировать инструменты и описывать их в правильном формате.

### 3. Проблемы с безопасностью

Нет встроенных механизмов для:

- Валидации параметров
- Контроля доступа
- Аудита вызовов
- Изоляции между инструментами

### 4. Сложность масштабирования

При росте количества инструментов становится сложно:

- Управлять их описаниями
- Обеспечивать консистентность
- Версионировать изменения
- Тестировать взаимодействия

## Вывод

Механизм tool calling предоставляет языковым моделям большие возможности по работе с окружением.

Однако подход с "ручным" определением инструментов имеет серьезные ограничения в плане стандартизации, переиспользуемости и масштабируемости.

Далее мы познакомимся с **Model Context Protocol (MCP)** - открытым стандартом, который решает все перечисленные проблемы и предоставляет унифицированный способ работы с инструментами, ресурсами и промптами для LLM-агентов.

MCP использует те же три этапа, что мы разобрали, но добавляет:

- Стандартизированный протокол обмена сообщениями
- Автоматическое обнаружение возможностей
- Встроенную валидацию и безопасность
- Возможность переиспользования инструментов между проектами
